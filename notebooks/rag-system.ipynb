{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f56d52cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-5.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.55.4-py3-none-any.whl.metadata (41 kB)\n",
      "Requirement already satisfied: tqdm in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from sentence-transformers) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers)\n",
      "  Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
      "Collecting scikit-learn (from sentence-transformers)\n",
      "  Downloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting scipy (from sentence-transformers)\n",
      "  Downloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (61 kB)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.34.4-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: Pillow in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from sentence-transformers) (4.15.0)\n",
      "Requirement already satisfied: filelock in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.5)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading fsspec-2025.7.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Downloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (703 bytes)\n",
      "Collecting sympy>=1.13.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.9.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.7.1 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
      "Collecting nvidia-nccl-cu12==2.27.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.8.90 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.4.0 (from torch>=1.11.0->sentence-transformers)\n",
      "  Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from triton==3.4.0->torch>=1.11.0->sentence-transformers) (80.9.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn->sentence-transformers)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading sentence_transformers-5.1.0-py3-none-any.whl (483 kB)\n",
      "Downloading transformers-4.55.4-py3-none-any.whl (11.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.34.4-py3-none-any.whl (561 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.5/561.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.1.8-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.7.0-py3-none-any.whl (199 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading torch-2.8.0-cp311-cp311-manylinux_2_28_x86_64.whl (888.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m888.1/888.1 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m  \u001b[33m0:01:43\u001b[0mm0:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m  \u001b[33m0:01:06\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (88.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m  \u001b[33m0:00:10\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (954 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (706.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m  \u001b[33m0:01:19\u001b[0mm0:00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (193.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m  \u001b[33m0:00:21\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (63.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (267.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m  \u001b[33m0:00:31\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (288.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m  \u001b[33m0:00:32\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (287.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m  \u001b[33m0:00:32\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (322.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.4/322.4 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m  \u001b[33m0:00:40\u001b[0mm0:00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (39.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
      "Downloading triton-3.4.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m  \u001b[33m0:00:19\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading networkx-3.5-py3-none-any.whl (2.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.7.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scipy-1.16.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (35.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.4/35.4 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: nvidia-cusparselt-cu12, mpmath, triton, threadpoolctl, sympy, scipy, safetensors, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, hf-xet, fsspec, scikit-learn, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, huggingface-hub, tokenizers, nvidia-cusolver-cu12, transformers, torch, sentence-transformers\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29/29\u001b[0m [sentence-transformers]ence-transformers]]2]\n",
      "\u001b[1A\u001b[2KSuccessfully installed fsspec-2025.7.0 hf-xet-1.1.8 huggingface-hub-0.34.4 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.3 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvtx-cu12-12.8.90 safetensors-0.6.2 scikit-learn-1.7.1 scipy-1.16.1 sentence-transformers-5.1.0 sympy-1.14.0 threadpoolctl-3.6.0 tokenizers-0.21.4 torch-2.8.0 transformers-4.55.4 triton-3.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from faiss-cpu) (2.3.2)\n",
      "Requirement already satisfied: packaging in /home/andrealombardo/GitHub/VLM-Hackathon/.venv/lib/python3.11/site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.12.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sentence-transformers\n",
    "%pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e703315",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import os\n",
    "import json\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "\n",
    "endpoint = \"https://oai-aip-cv-ont-sdc.openai.azure.com/\"\n",
    "model_name = \"gpt-4o-mini\"\n",
    "deployment = \"gpt-4o-mini\"\n",
    "token_provider = get_bearer_token_provider(DefaultAzureCredential(), \"https://cognitiveservices.azure.com/.default\")\n",
    "api_version = \"2024-12-01-preview\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=api_version,\n",
    "    azure_endpoint=endpoint,\n",
    "    azure_ad_token_provider=token_provider,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99664d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Sidewalk Defects VLM + RAG (Hackathon Notebook)\n",
    "\n",
    "Step 1: Indexing\n",
    "- Expect five grade folders `A+`, `A`, `B`, `C`, `D`\n",
    "- Each folder contains:\n",
    "  • one text file (any `*.txt`) with the grade description\n",
    "  • a few example images (`*.jpg|*.jpeg|*.png|*.webp`)\n",
    "\n",
    "Step 2: Retrieval\n",
    "- Given a query image:\n",
    "  • Embed it\n",
    "  • Retrieve the most similar grade example images and grade descriptions\n",
    "  • Use the retrieved grade as context for the LLM to answer: \"Which score would you give to this image?\"\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Any\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# =====================\n",
    "# Config\n",
    "# =====================\n",
    "PROJECT_ROOT = os.getcwd()\n",
    "GRADES_DIR = os.path.join(PROJECT_ROOT, \"data\", \"grades\")\n",
    "PREFERRED_MODEL = \"jinaai/jina-clip-v2\"\n",
    "FALLBACK_MODEL = \"clip-ViT-B-32\"\n",
    "\n",
    "# =====================\n",
    "# Data classes\n",
    "# =====================\n",
    "@dataclass\n",
    "class GradeDesc:\n",
    "    grade: str\n",
    "    description: str\n",
    "\n",
    "@dataclass\n",
    "class RetrievalResult:\n",
    "    idx: int\n",
    "    score: float\n",
    "    meta: Dict[str, str]\n",
    "\n",
    "# =====================\n",
    "# Utilities\n",
    "# =====================\n",
    "def l2_normalize(x: np.ndarray, axis: int = -1, eps: float = 1e-12) -> np.ndarray:\n",
    "    norm = np.linalg.norm(x, axis=axis, keepdims=True)\n",
    "    return x / np.maximum(norm, eps)\n",
    "\n",
    "def load_image(path: str) -> Image.Image:\n",
    "    return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "def walk_images(root: str) -> List[str]:\n",
    "    exts = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.webp\")\n",
    "    paths = []\n",
    "    for e in exts:\n",
    "        paths.extend(glob.glob(os.path.join(root, e)))\n",
    "    return sorted(paths)\n",
    "\n",
    "# =====================\n",
    "# Model\n",
    "# =====================\n",
    "def load_st_model() -> SentenceTransformer:\n",
    "    try:\n",
    "        return SentenceTransformer(PREFERRED_MODEL)\n",
    "    except Exception:\n",
    "        return SentenceTransformer(FALLBACK_MODEL)\n",
    "\n",
    "class Embedder:\n",
    "    def __init__(self, model: SentenceTransformer):\n",
    "        self.model = model\n",
    "\n",
    "    def embed_images(self, imgs: List[Image.Image]) -> np.ndarray:\n",
    "        return self.model.encode(imgs, convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
    "\n",
    "    def embed_texts(self, texts: List[str]) -> np.ndarray:\n",
    "        return self.model.encode(texts, convert_to_numpy=True, normalize_embeddings=True).astype(\"float32\")\n",
    "\n",
    "# =====================\n",
    "# Index\n",
    "# =====================\n",
    "class FaissIndex:\n",
    "    def __init__(self, dim: int):\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        self.meta: List[Dict[str, str]] = []\n",
    "        self.dim = dim\n",
    "\n",
    "    def add(self, vecs: np.ndarray, metas: List[Dict[str, str]]):\n",
    "        vecs = l2_normalize(vecs)\n",
    "        self.index.add(vecs.astype(\"float32\"))\n",
    "        self.meta.extend(metas)\n",
    "\n",
    "    def search(self, q: np.ndarray, topk: int = 5) -> List[RetrievalResult]:\n",
    "        \"\"\"Cosine similarity search; accepts 1D (d,) or 2D (n,d) query arrays.\n",
    "        Returns empty list if index is missing or empty.\n",
    "        \"\"\"\n",
    "        # Handle missing/empty index gracefully\n",
    "        if getattr(self, \"index\", None) is None:\n",
    "            return []\n",
    "        try:\n",
    "            ntotal = self.index.ntotal\n",
    "        except Exception:\n",
    "            ntotal = 0\n",
    "        if ntotal == 0:\n",
    "            return []\n",
    "\n",
    "        # Ensure 2D shape for FAISS\n",
    "        if q.ndim == 1:\n",
    "            q = q[None, :]\n",
    "        # Normalize (cosine via inner product)\n",
    "        if not np.allclose(np.linalg.norm(q, axis=1), 1.0, atol=1e-3):\n",
    "            q = l2_normalize(q)\n",
    "        # Search\n",
    "        scores, idxs = self.index.search(q.astype(\"float32\"), topk)\n",
    "        results: List[RetrievalResult] = []\n",
    "        for i, s in zip(idxs[0], scores[0]):\n",
    "            if i == -1:\n",
    "                continue\n",
    "            results.append(RetrievalResult(idx=int(i), score=float(s), meta=self.meta[int(i)]))\n",
    "        return results\n",
    "\n",
    "# =====================\n",
    "# Grade folder loader\n",
    "# =====================\n",
    "def read_grade_dirs(grades_root: str) -> Tuple[List[GradeDesc], List[str], List[Dict[str, str]]]:\n",
    "    grade_descs: List[GradeDesc] = []\n",
    "    img_paths: List[str] = []\n",
    "    img_metas: List[Dict[str, str]] = []\n",
    "\n",
    "    for grade in sorted(os.listdir(grades_root)):\n",
    "        gdir = os.path.join(grades_root, grade)\n",
    "        if not os.path.isdir(gdir):\n",
    "            continue\n",
    "        txts = glob.glob(os.path.join(gdir, \"*.txt\"))\n",
    "        print(f\"Found txts for {grade}: {txts}\")\n",
    "        desc = open(txts[0], \"r\", encoding=\"utf-8\").read().strip() if txts else f\"Grade {grade} (no description)\"\n",
    "        grade_descs.append(GradeDesc(grade=grade, description=desc))\n",
    "        for p in walk_images(gdir):\n",
    "            img_paths.append(p)\n",
    "            img_metas.append({\"grade\": grade, \"path\": p})\n",
    "\n",
    "    return grade_descs, img_paths, img_metas\n",
    "\n",
    "# =====================\n",
    "# Build indices\n",
    "# =====================\n",
    "def build_indices(embedder: Embedder, dim: int):\n",
    "    grade_descs, img_paths, img_metas = read_grade_dirs(GRADES_DIR)\n",
    "    print(grade_descs)\n",
    "\n",
    "    # Text index\n",
    "    texts = [f\"Grade {g.grade}: {g.description}\" for g in grade_descs]\n",
    "    text_embs = embedder.embed_texts(texts)\n",
    "    text_index = FaissIndex(dim)\n",
    "    text_index.add(text_embs, [{\"grade\": g.grade, \"text\": g.description} for g in grade_descs])\n",
    "\n",
    "    # Image index\n",
    "    imgs = [load_image(p) for p in img_paths]\n",
    "    img_embs = embedder.embed_images(imgs)\n",
    "    img_index = FaissIndex(dim)\n",
    "    img_index.add(img_embs, img_metas)\n",
    "\n",
    "    return text_index, img_index\n",
    "\n",
    "# =====================\n",
    "# Retrieval for a query image\n",
    "# =====================\n",
    "def retrieve_for_query(img_path: str, embedder: Embedder, text_index: FaissIndex, img_index: FaissIndex, topk: int = 3):\n",
    "    img = load_image(img_path)\n",
    "    q_emb = embedder.embed_images([img])\n",
    "\n",
    "    text_hits = text_index.search(q_emb, topk=topk)\n",
    "    img_hits = img_index.search(q_emb, topk=topk)\n",
    "\n",
    "    return text_hits, img_hits\n",
    "\n",
    "# =====================\n",
    "# Retrieval & LLM-context helpers\n",
    "# =====================\n",
    "from collections import defaultdict\n",
    "\n",
    "@dataclass\n",
    "class GradeDecision:\n",
    "    chosen_grade: str\n",
    "    fused_scores: Dict[str, float]\n",
    "    img_best_by_grade: Dict[str, Tuple[str, float]]  # grade -> (path, score)\n",
    "    top_img_hits: List[RetrievalResult]\n",
    "    top_text_hits: List[RetrievalResult]\n",
    "\n",
    "\n",
    "def embed_query_image(path: str, embedder: Embedder) -> np.ndarray:\n",
    "    img = load_image(path)\n",
    "    return embedder.embed_images([img])[0]\n",
    "\n",
    "\n",
    "def retrieve_candidates(\n",
    "    q_emb: np.ndarray,\n",
    "    img_index: FaissIndex,\n",
    "    text_index: FaissIndex,\n",
    "    per_grade_top: int = 1,\n",
    "    topk_img_global: int = 5,\n",
    "    topk_text: int = 5,\n",
    ") -> Tuple[Dict[str, List[Tuple[str, float]]], List[RetrievalResult], List[RetrievalResult]]:\n",
    "    \"\"\"Returns:\n",
    "    - best_per_grade: dict grade -> list of (path, score) for the top image examples within that grade\n",
    "    - top_img_global: top-K image hits across all grades\n",
    "    - top_text_hits: top-K text description hits\n",
    "    \"\"\"\n",
    "    # global image hits (guard if index empty)\n",
    "    top_img_global = img_index.search(q_emb, topk=topk_img_global) if getattr(img_index, \"index\", None) else []\n",
    "\n",
    "    # group best per grade\n",
    "    per_grade = defaultdict(list)\n",
    "    if getattr(img_index, \"index\", None):\n",
    "        for hit in img_index.search(q_emb, topk=max(50, topk_img_global)):\n",
    "            g = hit.meta.get(\"grade\", \"?\")\n",
    "            per_grade[g].append((hit.meta.get(\"path\", \"\"), hit.score))\n",
    "    best_per_grade = {g: sorted(v, key=lambda x: x[1], reverse=True)[:per_grade_top] for g, v in per_grade.items()}\n",
    "\n",
    "    # text hits (guard if index empty)\n",
    "    top_text_hits = text_index.search(q_emb, topk=topk_text) if getattr(text_index, \"index\", None) else []\n",
    "\n",
    "    return best_per_grade, top_img_global, top_text_hits\n",
    "\n",
    "\n",
    "def decide_grade(\n",
    "    best_per_grade: Dict[str, List[Tuple[str, float]]],\n",
    "    top_text_hits: List[RetrievalResult],\n",
    "    weight_img: float = 0.6,\n",
    "    weight_text: float = 0.4,\n",
    ") -> Tuple[str, Dict[str, float]]:\n",
    "    # image scores: take the best score within each grade\n",
    "    img_scores: Dict[str, float] = {}\n",
    "    for g, items in best_per_grade.items():\n",
    "        if items:\n",
    "            img_scores[g] = max(s for _, s in items)\n",
    "\n",
    "    # text scores: keep best score per grade\n",
    "    text_scores: Dict[str, float] = {}\n",
    "    for h in top_text_hits:\n",
    "        g = h.meta.get(\"grade\", \"?\")\n",
    "        text_scores[g] = max(text_scores.get(g, -1.0), h.score)\n",
    "\n",
    "    all_grades = sorted(set(list(img_scores.keys()) + list(text_scores.keys())))\n",
    "    fused: Dict[str, float] = {}\n",
    "    for g in all_grades:\n",
    "        fused[g] = weight_img * img_scores.get(g, 0.0) + weight_text * text_scores.get(g, 0.0)\n",
    "\n",
    "    if not fused:\n",
    "        return \"N/A\", {}\n",
    "\n",
    "    # normalize for readability\n",
    "    vals = np.array(list(fused.values()), dtype=float)\n",
    "    if vals.max() > 0:\n",
    "        vals = (vals - vals.min()) / (vals.max() - vals.min() + 1e-6)\n",
    "        for g, v in zip(list(fused.keys()), vals.tolist()):\n",
    "            fused[g] = float(v)\n",
    "\n",
    "    chosen = max(fused.items(), key=lambda kv: kv[1])[0]\n",
    "    return chosen, fused\n",
    "\n",
    "\n",
    "def make_llm_context(\n",
    "    query_image_path: str,\n",
    "    decision: GradeDecision,\n",
    "    max_examples: int = 3,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Prepare a compact context payload you can pass to your LLM alongside the user question.\"\"\"\n",
    "    # take up to N example images (favor the chosen grade first)\n",
    "    chosen = decision.chosen_grade\n",
    "    examples: List[Dict[str, Any]] = []\n",
    "\n",
    "    # chosen-grade example(s)\n",
    "    if chosen in decision.img_best_by_grade:\n",
    "        for path, sc in [decision.img_best_by_grade[chosen]]:\n",
    "            examples.append({\"grade\": chosen, \"path\": path, \"similarity\": sc})\n",
    "\n",
    "    # fill with top global hits (diverse grades)\n",
    "    seen = {e[\"path\"] for e in examples}\n",
    "    for h in decision.top_img_hits:\n",
    "        if len(examples) >= max_examples:\n",
    "            break\n",
    "        p = h.meta.get(\"path\", \"\")\n",
    "        if p and p not in seen:\n",
    "            examples.append({\"grade\": h.meta.get(\"grade\", \"?\"), \"path\": p, \"similarity\": h.score})\n",
    "            seen.add(p)\n",
    "\n",
    "    # top text descriptions (trim)\n",
    "    texts = []\n",
    "    for h in decision.top_text_hits[:3]:\n",
    "        texts.append({\"grade\": h.meta.get(\"grade\",\"?\"), \"text\": h.meta.get(\"text\",\"\")[:240], \"similarity\": h.score})\n",
    "\n",
    "    return {\n",
    "        \"query_image\": query_image_path,\n",
    "        \"proposed_grade\": chosen,\n",
    "        \"fused_scores\": decision.fused_scores,\n",
    "        \"example_images\": examples,\n",
    "        \"grade_text_matches\": texts,\n",
    "        \"instructions\": (\n",
    "            \"Decide the sidewalk grade (A+, A, B, C, D). Use the example images and grade descriptions as guidance. \"\n",
    "            \"Prefer visual similarity; use text to break ties. Explain briefly.\"\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "def grade_query_image(\n",
    "    query_image_path: str,\n",
    "    embedder: Embedder,\n",
    "    text_index: FaissIndex,\n",
    "    img_index: FaissIndex,\n",
    "    weight_img: float = 0.6,\n",
    "    weight_text: float = 0.4,\n",
    ") -> Tuple[str, Dict[str, Any]]:\n",
    "    \"\"\"High-level helper: embed → retrieve → fuse → pack LLM context.\n",
    "    Returns (chosen_grade, context_dict).\n",
    "    \"\"\"\n",
    "    q_emb = embed_query_image(query_image_path, embedder)\n",
    "    best_per_grade, top_img_global, top_text_hits = retrieve_candidates(q_emb, img_index, text_index)\n",
    "\n",
    "    # map best per grade into single (path,score) for context\n",
    "    best_single = {g: v[0] for g, v in best_per_grade.items() if v}\n",
    "\n",
    "    chosen, fused = decide_grade(best_per_grade, top_text_hits, weight_img=weight_img, weight_text=weight_text)\n",
    "    decision = GradeDecision(\n",
    "        chosen_grade=chosen,\n",
    "        fused_scores=fused,\n",
    "        img_best_by_grade=best_single,\n",
    "        top_img_hits=top_img_global,\n",
    "        top_text_hits=top_text_hits,\n",
    "    )\n",
    "    ctx = make_llm_context(query_image_path, decision)\n",
    "    return chosen, ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "200fddfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building indices...\n",
      "Found txts for A: ['/home/andrealombardo/GitHub/VLM-Hackathon/notebooks/data/grades/A/description.txt']\n",
      "Found txts for A+: ['/home/andrealombardo/GitHub/VLM-Hackathon/notebooks/data/grades/A+/DescriptionA+.txt']\n",
      "Found txts for B: ['/home/andrealombardo/GitHub/VLM-Hackathon/notebooks/data/grades/B/description.txt']\n",
      "Found txts for C: ['/home/andrealombardo/GitHub/VLM-Hackathon/notebooks/data/grades/C/description.txt']\n",
      "Found txts for D: ['/home/andrealombardo/GitHub/VLM-Hackathon/notebooks/data/grades/D/description.txt']\n",
      "[GradeDesc(grade='A', description='Er ontbreken geen elementen uit de verharding. Er zijn nauwelijks beschadigde elementen.\\t\\nBeschadigde elementen\\t1'), GradeDesc(grade='A+', description='Er ontbreken geen elementen uit de verharding. Er zijn geen beschadigde elementen.\\t\\nBeschadigde elementen: 0'), GradeDesc(grade='B', description='Er ontbreken geen elementen uit de verharding. Er zijn hier en daar beschadigde elementen.\\t\\nBeschadigde elementen\\t2-3'), GradeDesc(grade='C', description='Er ontbreken incidenteel elementen uit de verharding. Er zijn regelmatig beschadigde elementen.\\t\\nBeschadigde elementen\\t4 of 5'), GradeDesc(grade='D', description='Er ontbreken elementen uit de verharding. Er zijn veel beschadigde elementen.\\t\\nBeschadigde elementen\\t6 of meer')]\n",
      "\n",
      "Text hits:\n",
      "  Grade=A+ | score=0.263 | desc=Er ontbreken geen elementen uit de verharding. Er zijn geen …\n",
      "  Grade=D | score=0.263 | desc=Er ontbreken elementen uit de verharding. Er zijn veel besch…\n",
      "  Grade=B | score=0.262 | desc=Er ontbreken geen elementen uit de verharding. Er zijn hier …\n",
      "\n",
      "Image hits:\n",
      "  Grade=C | score=0.891 | img=beschadigde elementen_C.jpg\n",
      "  Grade=A | score=0.872 | img=PXL_20250826_095437041.NIGHT.jpg\n",
      "  Grade=D | score=0.856 | img=beschadigde elementen_D.jpg\n"
     ]
    }
   ],
   "source": [
    "# =====================\n",
    "# Demo\n",
    "# =====================\n",
    "if __name__ == \"__main__\":\n",
    "    model = load_st_model()\n",
    "    embedder = Embedder(model)\n",
    "    dim = embedder.embed_texts([\"probe\"]).shape[1]\n",
    "\n",
    "    print(\"Building indices...\")\n",
    "    text_index, img_index = build_indices(embedder, dim)\n",
    "\n",
    "    # Example query\n",
    "    images_folder = \"local_data/2025_Centrum/images/\"  # See other notebook on how to download an image from blob store\n",
    "    image_name = \"071a94bf-5563-4126-8684-dc73b3ab2025.jpeg\"\n",
    "    image_name_path = os.path.join(images_folder, image_name)\n",
    "    sample_query = os.path.join(image_name_path)\n",
    "    if os.path.exists(sample_query):\n",
    "        t_hits, i_hits = retrieve_for_query(sample_query, embedder, text_index, img_index)\n",
    "        print(\"\\nText hits:\")\n",
    "        for h in t_hits:\n",
    "            print(f\"  Grade={h.meta['grade']} | score={h.score:.3f} | desc={h.meta['text'][:60]}…\")\n",
    "        print(\"\\nImage hits:\")\n",
    "        for h in i_hits:\n",
    "            print(f\"  Grade={h.meta['grade']} | score={h.score:.3f} | img={os.path.basename(h.meta['path'])}\")\n",
    "    else:\n",
    "        print(\"No sample query image found at\", sample_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef3c3fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen grade: C\n",
      "LLM context: {\n",
      "  \"query_image\": \"local_data/2025_Centrum/images/071a94bf-5563-4126-8684-dc73b3ab2025.jpeg\",\n",
      "  \"proposed_grade\": \"C\",\n",
      "  \"fused_scores\": {\n",
      "    \"A\": 0.7691913652137267,\n",
      "    \"A+\": 0.0,\n",
      "    \"B\": 0.40345896546843774,\n",
      "    \"C\": 0.9999813645165238,\n",
      "    \"D\": 0.6303082055727589\n",
      "  },\n",
      "  \"example_images\": [\n",
      "    {\n",
      "      \"grade\": \"C\",\n",
      "      \"path\": \"/home/andrealombardo/GitHub/VLM-Hackathon/notebooks/data/grades/C/beschadigde elementen_C.jpg\",\n",
      "      \"similarity\": 0.8905883431434631\n",
      "    },\n",
      "    {\n",
      "      \"grade\": \"A\",\n",
      "      \"path\": \"/home/andrealombardo/GitHub/VLM-Hackathon/notebooks/data/grades/A/PXL_20250826_095437041.NIGHT.jpg\",\n",
      "      \"similarity\": 0.8723095059394836\n",
      "    },\n",
      "    {\n",
      "      \"grade\": \"D\",\n",
      "      \"path\": \"/home/andrealombardo/GitHub/VLM-Hackathon/notebooks/data/grades/D/beschadigde elementen_D.jpg\",\n",
      "      \"similarity\": 0.8564962148666382\n",
      "    }\n",
      "  ],\n",
      "  \"grade_text_matches\": [\n",
      "    {\n",
      "      \"grade\": \"A+\",\n",
      "      \"text\": \"Er ontbreken geen elementen uit de verharding. Er zijn geen beschadigde elementen.\\t\\nBeschadigde elementen: 0\",\n",
      "      \"similarity\": 0.2634483277797699\n",
      "    },\n",
      "    {\n",
      "      \"grade\": \"D\",\n",
      "      \"text\": \"Er ontbreken elementen uit de verharding. Er zijn veel beschadigde elementen.\\t\\nBeschadigde elementen\\t6 of meer\",\n",
      "      \"similarity\": 0.2630348205566406\n",
      "    },\n",
      "    {\n",
      "      \"grade\": \"B\",\n",
      "      \"text\": \"Er ontbreken geen elementen uit de verharding. Er zijn hier en daar beschadigde elementen.\\t\\nBeschadigde elementen\\t2-3\",\n",
      "      \"similarity\": 0.2616399824619293\n",
      "    }\n",
      "  ],\n",
      "  \"instructions\": \"Decide the sidewalk grade (A+, A, B, C, D). Use the example images and grade descriptions as guidance. Prefer visual similarity; use text to break ties. Explain briefly.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "chosen, ctx = grade_query_image(sample_query, embedder, text_index, img_index)\n",
    "print(\"Chosen grade:\", chosen)\n",
    "print(\"LLM context:\", json.dumps(ctx, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67d68228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"category1\": \"hole\",\n",
      "    \"category2\": \"unfilled borehole\",\n",
      "    \"description\": \"The image shows a significant depression in the sidewalk with a visible gap in the paving bricks, indicating a hole that needs to be filled with new tiles or repairs.\",\n",
      "    \"reasoning_steps\": \"Upon examining the image, there is a clear depression in the surface with an absence of paving elements. This aligns best with the 'hole' category, while nearby exposed ground could suggest an unfilled borehole as a secondary option.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Function to encode an image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "images_folder = \"local_data/2025_Centrum/images/\"  # See other notebook on how to download an image from blob store\n",
    "image_name = \"ecf38198-dabd-4895-ac67-6478d1e5cc53.jpeg\"\n",
    "image_path = os.path.join(images_folder, image_name)\n",
    "base64_image = encode_image(os.path.join(images_folder, image_name))\n",
    "\n",
    "prompt = \"You are a sidewalk inspector for the municipality. Your task is \" \\\n",
    "\"to check whether there is any damage on the sidewalk. \" \\\n",
    "\"If so, choose one or multiple categories of damage from the following list: \"\n",
    "\n",
    "list_defect = [\n",
    "  \"Local subsidence\",\n",
    "  \"Edge damage\",\n",
    "  \"Tree root damage\",\n",
    "  \"Transverse crack\",\n",
    "  \"Broken elements\",\n",
    "  \"Unfilled borehole\",\n",
    "  \"Hole\",\n",
    "  \"Local raise\",\n",
    "  \"Longitudinal crack\",\n",
    "  \"Asphalt trench elements (HOOR)\",\n",
    "  \"Square inspection chamber connection\",\n",
    "  \"Surface connection\",\n",
    "  \"Fraying\",\n",
    "  \"Edge restraint\",\n",
    "  \"Loose elements\",\n",
    "  \"Failed area\",\n",
    "  \"Joint width of elements\",\n",
    "  \"Missing elements\"\n",
    "]\n",
    "\n",
    "content = f\"{prompt} {list_defect}. Please answer with only the categories in the list. If you don't find any defects, please say so. In any case, provide an explanation.\"\n",
    "\n",
    "schadebeel = ['local subsidence', 'edge damage', 'tree root damage',\n",
    "'transverse crack', 'broken elements', 'unfilled borehole', 'hole',\n",
    "'local raise', 'longitudinal crack',\n",
    "'asphalt trench elements (HOOR)',\n",
    "'square inspection chamber connection', 'gully connection',\n",
    "'fraying', 'edge restraint', 'loose elements',\n",
    "'failed area', 'joint width of elements', 'missing elements']\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a highly skilled sidewalk inspection assistant.\n",
    "You work for Amsterdam, a city where soil subsidence is a major issue causing sidewalks to often be depressed because of the underlying soil conditions.\n",
    "\n",
    "Before classifying an image, follow this reasoning process:\n",
    "\n",
    "1. Examine the image carefully and detect any visible damage, cracks, or irregularities.\n",
    "2. Decide which category from {schadebeel} best fits the observed damage. For example, if there are huge gaps, it would be a hole which needs new tiles to be placed.\n",
    "3. Think what kind of maintenance needs to be carried out to fix the category. This could further help determine the category. Also, if its close to a treet, there are roots, it could be tree root damage or if its close to a bollard, it could be loose elements or gap.\n",
    "4. Provide a short descriptive text explaining why you chose that category. Give me also a second likely option.\n",
    "5. Output your final answer strictly in JSON format:\n",
    "\n",
    "{{\n",
    "    \"category1\": \"<first estimation based on category>\",\n",
    "    \"category2\": \"<second estimation based on category>\",\n",
    "    \"description\": \"<short text explaining your assessment>\",\n",
    "    \"reasoning_steps\": \"<optional, internal reasoning for traceability>\"\n",
    "}}\n",
    "\n",
    "Important:\n",
    "- Do not skip the reasoning step; it should briefly summarize your assessment process.\n",
    "- Return only valid JSON with the three keys.\n",
    "\"\"\"\n",
    "\n",
    "# Query endpoint\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": system_prompt,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"text\", \"text\": \"Find if there is any damage in this image.\" },\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                    \"detail\": \"low\", # reduces token usage\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    model=deployment\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f1cd0232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using RAG system to decide the grade...\n",
      "Chosen grade: C\n",
      "LLM context: {\n",
      "  \"query_image\": \"local_data/2025_Centrum/images/ecf38198-dabd-4895-ac67-6478d1e5cc53.jpeg\",\n",
      "  \"proposed_grade\": \"C\",\n",
      "  \"fused_scores\": {\n",
      "    \"A\": 0.8383898689905828,\n",
      "    \"A+\": 0.0,\n",
      "    \"B\": 0.7489669681978307,\n",
      "    \"C\": 0.9999503374031107,\n",
      "    \"D\": 0.8255494541812297\n",
      "  },\n",
      "  \"example_images\": [\n",
      "    {\n",
      "      \"grade\": \"C\",\n",
      "      \"path\": \"/home/andrealombardo/GitHub/VLM-Hackathon/notebooks/data/grades/C/beschadigde elementen_C.jpg\",\n",
      "      \"similarity\": 0.8655822277069092\n",
      "    },\n",
      "    {\n",
      "      \"grade\": \"B\",\n",
      "      \"path\": \"/home/andrealombardo/GitHub/VLM-Hackathon/notebooks/data/grades/B/PXL_20250826_094441911.jpg\",\n",
      "      \"similarity\": 0.8767260313034058\n",
      "    },\n",
      "    {\n",
      "      \"grade\": \"A\",\n",
      "      \"path\": \"/home/andrealombardo/GitHub/VLM-Hackathon/notebooks/data/grades/A/PXL_20250826_095437041.NIGHT.jpg\",\n",
      "      \"similarity\": 0.858893096446991\n",
      "    }\n",
      "  ],\n",
      "  \"grade_text_matches\": [\n",
      "    {\n",
      "      \"grade\": \"D\",\n",
      "      \"text\": \"Er ontbreken elementen uit de verharding. Er zijn veel beschadigde elementen.\\t\\nBeschadigde elementen\\t6 of meer\",\n",
      "      \"similarity\": 0.25651001930236816\n",
      "    },\n",
      "    {\n",
      "      \"grade\": \"C\",\n",
      "      \"text\": \"Er ontbreken incidenteel elementen uit de verharding. Er zijn regelmatig beschadigde elementen.\\t\\nBeschadigde elementen\\t4 of 5\",\n",
      "      \"similarity\": 0.2551908493041992\n",
      "    },\n",
      "    {\n",
      "      \"grade\": \"A\",\n",
      "      \"text\": \"Er ontbreken geen elementen uit de verharding. Er zijn nauwelijks beschadigde elementen.\\t\\nBeschadigde elementen\\t1\",\n",
      "      \"similarity\": 0.2527966797351837\n",
      "    }\n",
      "  ],\n",
      "  \"instructions\": \"Decide the sidewalk grade (A+, A, B, C, D). Use the example images and grade descriptions as guidance. Prefer visual similarity; use text to break ties. Explain briefly.\"\n",
      "}\n",
      "The sidewalk shows significant damage, specifically categorized as 'hole' and 'unfilled borehole'. The image depicts a large depression in the sidewalk with missing paving bricks, exposing the ground underneath. This aligns with the 'hole' category due to the clear absence of paving elements, while the exposed area suggests an unfilled borehole as a secondary issue.\n",
      "\n",
      "Based on the provided context and the severity of the damage, I would assign a grade of 'C'. This grade reflects that there are regularly damaged elements and occasional missing elements in the pavement, as indicated by the presence of the hole and the unfilled borehole. The visual similarity to the example images that also received a grade of 'C' supports this decision.\n"
     ]
    }
   ],
   "source": [
    "prompt_2 = \"\"\n",
    "\n",
    "# if the previous answer contains hole or missing elements, we use the RAG-system to decide the grade\n",
    "if \"hole\" in response.choices[0].message.content.lower() or \"missing elements\" in response.choices[0].message.content.lower():\n",
    "    print(\"Using RAG system to decide the grade...\")\n",
    "    chosen, ctx = grade_query_image(image_path, embedder, text_index, img_index)\n",
    "    print(\"Chosen grade:\", chosen)\n",
    "    print(\"LLM context:\", json.dumps(ctx, indent=2))\n",
    "\n",
    "    prompt = f\"The previous answer was {response.choices[0].message.content}. \" \\\n",
    "    f\"Now, based on the retrieved context: {ctx}\" \\\n",
    "    \"Decide the sidewalk grade (A+, A, B, C, D). Use the example images and grade descriptions as guidance. Prefer visual similarity; use text to break ties. Explain briefly. \"\n",
    "    \n",
    "    prompt_2 = prompt\n",
    "else:\n",
    "    prompt = f\"The previous answer was {response.choices[0].message.content}. \" \\\n",
    "    \"Decide the sidewalk grade (A+, A, B, C, D). Use your own knowledge and reasoning. Explain your decision briefly. \"\n",
    "    prompt_2 = prompt\n",
    "    print(\"No hole or missing elements found, using LLM reasoning only. Prompt for final grading:\", prompt_2)\n",
    "\n",
    "# Query endpoint\n",
    "response_2 = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": content,\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"text\", \"text\": f\"{prompt_2}\"},\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"},\n",
    "                    \"detail\": \"low\", # reduces token usage\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=0.3,\n",
    "    top_p=1.0,\n",
    "    model=deployment\n",
    ")\n",
    "\n",
    "print(response_2.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ba2bf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VLM-Hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
